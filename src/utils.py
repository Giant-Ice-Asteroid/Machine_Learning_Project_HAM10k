import os
import torch
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import pandas as pd
import random

############### collection of helpful utility functions ######################


def set_seed(seed=42):
    """
    sets random seeds for reproducibility across libraries
    
    a "seed" is the starting point that determines which particular sequence will be generated when generating a "random" number
    -> "random" numbers in computing are generated by mathematical formulas called pseudorandom number generators 
    ---> they produce sequences of numbers that appear random, but actaully aren't
    -----> same seed -> same "random" results
    """
    torch.manual_seed(seed) #sets the random seed for PyTorch CPU operations
    torch.cuda.manual_seed_all(seed) # sets seed for all CUDA devices
    np.random.seed(seed) # set numpy's random seed
    random.seed(seed) # sets python built in librbay rnadom seed
    torch.backends.cudnn.deterministic = True # this makes CUDA operations "deterministic" (forces it to produce same results every time)
    

def create_log_dir():
    # Create a directory for logs and checkpoints if it doens't exist already
    log_dir = os.path.join('logs', datetime.now().strftime('%Y%m%d_%H%M%S')) #Creates a uniquely named directory based on the current date and time (e.g., "logs/20250506_142030")
    os.makedirs(log_dir, exist_ok=True) #Creates this directory if it doesn't exist (exist_ok=True prevents errors if it already exists)
    return log_dir

def save_config(config, log_dir):
    """ 
    saves configuration parameters to a txt file
    """
    with open(os.path.join(log_dir, "config.txt"), "w") as f: # open a file in write mode in the log dir
        for key, value in config.items(): # iterates through eahc key-val pair in the config dict 
            f.write(f"{key}: {value}\n") # and write sthem as key:value on a new line
            
def load_checkpoint(model, checkpoint_path):
    """
    function that loads a saved checkpoint file 
    can be used for resuming training or loading a pretrained model for eval
    """
    checkpoint = torch.load(checkpoint_path) # loads saved checkpoint into memory
    model.load_state_dict(checkpoint["model_state_dict"]) # restores model weights from the checkpoint in question
    return model, checkpoint["epoch"] # return the loaded model as well as the epoch number

def plot_class_distribution(df, target_column, title, log_dir):
    """Plot class distribution as a bar chart"""
    plt.figure(figsize=(10, 6))
    class_counts = df[target_column].value_counts().sort_index()#Counts occurrences of each class and sorts them
    
    plt.bar(class_counts.index, class_counts.values) #creates the bar chart

    plt.xlabel('Class')
    plt.ylabel('Count')
    plt.title(title)
    plt.xticks(class_counts.index)
    
    # Add count labels on top of each bar
    for i, v in enumerate(class_counts.values): 
        plt.text(class_counts.index[i], v + 5, str(v), ha='center')
    
    plt.savefig(os.path.join(log_dir, f"{title.lower().replace(' ', '_')}.png"))
    plt.show()